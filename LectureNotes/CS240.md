# CS240 Notes

## Lecture 1 

Assignment 0 due May 15th 

Eric Schost 

Admin stuff:

50% Final

25% Mid Term

5 assignment each 5% 

ADT, abstract data type

how to implement efficiently using DS

mathematical analysis 

psuedo-code, analyzed using order notation 

big Oh
 
priority QUeue, heaps 

sorting, selection 

AVL trees, B trees 

skip lists 

hashing 

quadtres, KD-trees

range search 

tried 

string matching 

data compression 

Problem, count positive integers in an array

An instance : [1,2,3,4,5]

solution: 15

size of an instance(length of the array)

Initialize a counter to zero, one pass over array, for any entry A[i] if A[i] > 0, increment counter 


or 
```

count(a)

res = 0 
for i = 0... n-1,
if A[i] > 0
    res ++
return res

```

running time and auxiliary space 

amount of time/memory are dependent on Size(I)

Running Time Analysis:

No implementing

Independent of Environment

All input instances 

Random Access Machine MOdel

idealied model 

order notation 

ignore constants and lower order terms 

Order Notation 

F(n) = 75n + 500 
g(n) = 5n2

1. for n >= 20 25*20 <= 25n
2. for n >= 20 20n < n2 -> 100n <= 5n2

first principle proof

2n2 + 3n + 11 <= cn2 for all n >= n0

for n>=1 2n^2 < 2n^2 

3n <= 3n^2

11 <= 11n^2

f(n) <= 16n^2 = 16g(n)

so, taking n0 = 1 c = 16, this proves f(n) belongs O(g(n))

## Lecture 2 5/9

find C and n0 such that it is satisfied 

omega notation exists c > 0 n0 > 0 such that c|g(n)| <= \f(n)\

theta notation 

theta means f(n) is both O(gn) and Omega(gn)

tight bound 

omega is lower bound 

f(n) belongs to Omega(g(n))

there exists C and n0 such that both > 0 

for n >= n0 f(n) >= c g(n) lower bound on run time 

theta means both omega and Big O 

prove f(n) 2n2 + 3n + 11 belongs to Omega(n2)

for all n >= 1 2n^2 >= 2n^2
3n >= 0
11 >= 0

f(n) >= 2n2 = 2g(n)
taking c = 2 and n0 = 1,that proves f(n) belongs Omega(n)

1/2 n^2 - 5n belongs omega(n^2)

f(n) = 1/2 n^2 -5n
g(n) = n^2 

let n >= 20 n^2 >= 20n 
1/4 n^2 >= 5n 
```
-5n >=  -1/4 n^2
1/2n^2 - 5n >= -1/4 n^2 + 1/2n^2 i.e 1/4 n^2 

```
f(n) >= 1/4 g(n) absolute values 

therefore by FP, Omega true 

Taking n0 = 20 and c = 1/4 thus proves Omega 

prove logb(n) belongs to theta(log n) for all b > 1 from first principles 

f(n) = logb(n) log(n)/log(b)
g(n) = log(n)

1/logb g(n) <= f(n) <= 1/logb g(n) taking n0 = 1, c1 = c2 = 1/logb 

strictly smaller larger asymptotic bounds 

small o notation 

f(n) belongs o(g(n)) if for all constant c > 0, ther eexists a constant n0 > 0 such that ... 

small omega notation 

f(n) belongs to small(g(N)) if g(n) belongs to o(f(n))

reverse of small o 

little o example 

f(n) 2000n^2 

g(n) n^n 

f(n) is little o of g(n)

given c > 0 find n0 such that n > n0 2000n^2 < cn^n

2000 < c n^n-2 

for n >= 3 n-2 >-1 so n <= n^n-2

for n>= 3 and n >= 2000/c + 1 

2000/c < 2000/c + 1 <= n <= n^n-2 

so 2000 < cn^n-2 

takig n0 = max(3, 2000/c + 1 )
thus proves ....

Algebra of Order notations 

f(n) belongsto Theta(f(n))

Maximum rules 

O(fn + gn) = O(max(fn, gn))
Omega(fn + gn) = Omega(max(fn, gn))

Transitivity 

fn = O(gn), gn = O(hn) => fn = O(hn)

similar with Omega
 
Techniques for Order notation 

L = lim n->infinity fn/gn 

f(n) > 0 and g(n) > 0 for all n >= n0

then f(n) is 

o(gn) if L = 0
Theta(gn) if 0 < L < inifinity 
small omega(gn) if L = infinity 

Lhopital's rule 

sufficient conditions 

let fn be a polynomial of degree d >= 0

cd n^d + ..  + c0

for some cd > 0

f(n) belongs to Theta(n^d)

fn/gn = cd + cd1 * 1/n ... 

all goes to 0 except cd, a constant

so theta(gn) is true

prove n(2 + sin n pie /2 ) is theta(n)

limit does not exist

for n >= 1 

-1 <= sin(n pie / 2) <= 1 

n <= n(2+sin(...)) <= 3n 

//first principle proof

compare the growth rate of log n and n 

logN ^ c and n ^ d 

f(n) = log(n)
g(n) = n

want f(n) belongs to o(g(n))

limit test 

L'hopital 

f(n) = ln(n) / ln(2) so f'n = 1/nln(2)

g'(n) = 1 

## Tutorial 1 

Latex and Asymptotic Analysis 

Zach TA 

zfrenett@uwaterloo.ca

pdflatex

TexStudio or Miktex

```
\documentclass[12pt][article]

\usepackage[]

amsmath, amsfonts
graphicx 
tikz for trees 


\begin{document}

\\ newline 

\large
\bf bold font

\vspace vertical space
\hspace horizontal space 

\section*{abc} section not numbered
\setion{abc} numbered 


$ \log(n!) \in \Theta(n \log n) $ \\ inline math 
\Theta vs \theta or \omega vs \Omega
\exists 
\forall


\begin{equation*} without number with *
\end{equation} 

$ or \begin{equation}

\lvert \rvert vertical line right and left 

\leq left equal 
\geq 

c_1 subscript for c with 1
c^1 means super script
\sum_{i = 1}^{n} for the summation 


\begin{align*} for aligning multiple equations

& for aligning equations
&= 

\prod product 


```

prove from first principels that

$$ 2^{\sqrt{logn}} \in o(n) $$

show that forall c > 0, exists n0 > 0, such that 
$$ |2^{\sqrt{logn}}| < c|n| \hspace{1cm}  \forall n >= n0 $$

prove 
$$2^{\sqrt{logn}}| < \sqrt{n} < c|n| $$

for n >= 17

16 < n 

4 leq logn
2 leq sqrt(logn)

2 sqrt(logn) < log(n)



 2^\sqrt{logn} < 2^log_4{n} = sqrt(n) 

 

## Lecture 4 


$$ log(n) \in o(n) $$ 

similarly, 

$$ log(n) \in o(n^a) $$
for a > 0

f(n) = log(n) => f'(n) = 1/ln2 * 1/n

g(n) = n ^ a  => g'(n) = ...

so f'(n) / g'(n) = 1/ ln2 * 1/a * 1/n^a

lim n -> infinity f'n/g'n = 0 ...


finally, we prove 

$$ log(n) ^ c \in o(n^d) \hspace{1cm} for c,d > 0 $$

$$ log(n)^c/n^d = log(n)^c / n^{(d/c)^c} $$

$$ = (log(n) / n^{d/c})^c $$

in the previous case with a = d/c 

proved 

$$ lim \infin log(n)/n^{d/c} = 0 $$

so lim.. = 0 

by limit test, 

log(n) ^ c in o(n^d)


little o for growth rate smaller 

theta the same 

small omega growth rate of f(n) is greater than growth rate of g(n)

common growth rate -- in lecture notes

### Analysis of Algorithm 2

nested loop

def T1(n) be the runtime of test1(n)

Then 

$$ T1(n) \in \theta(s1(n)) $$ 

where s1(n) is the number of times we go through step 4 

$$ s1(n) = \sum_{1 - n}^n \sum_{j = i}^n 1 $$

term 1 = n - i + 1 
term 2 outer loop sub it as n - i + 1 

= ... 

= n^2 - n(n+1)/2 + n
= 1/2 n^2 + 1/2n in O(n^2)

so T1(n) in theta(n^2)

s1(n) \leq sum 1 - n sum 1 - n (1) = n ^2 

so O(n^2) is confirmed 

s1(n) is number of integer points in ..

get omega from 1/4 N^2 

```
s1(n) >= sum 1 - n/2 sum i - n 1 

>= sum 1 - n/2 sum n/2 - n 1 >= n^2/4

thus omega(n^2)
```

O() and omega() proves s1(n) belongs to theta(n^2)

T2(n) belongs to theta(S2(N))

S2(n) is the number of times we go to step 6



then...


S2(n) = 

$$ \sum_{i = 1}^n \sum_{j = i}^n \sum_{k=i}^j 1 $$
can prove s2(n) = ...... 

can be proven easily that s2(n) <= n^3 

and simplify 

s2(n) >= sum 1 - n/3 sum i - n sum i - j (i)

s2(n) >= sum 1 - n/3 sum 2n/3 - n sum i - j (i)

sum i - j >= n/3 if i <= n/3  and j >= 2n/3 

s2(n) >= (n/3)^3 

while loop determines Ta(I) 

Test2(A,n) sorts A in decreasing order theta(n^2)

worst case A is sorted in increasing order theta(n)


average case complexity of an algo


## Lecture 5 

Merge Sort 

A of n integers : 

split A into 2 sub arrays 

recursively run merge sort on A l and A r 

after Al and Ar sorted, use a function merge to merge them into a single sorted array 

see slide for Merge sort implementation 

Mergesort ... 

Merge  ..

both sub arrays sorted 

Merge taeks time Theta(r - l + 1) i.e Theta(n) time for merging n elements 

bounded 

```
n = 5 
l = 0 
r = n - 1 = 4 
m = l + r / 2  = 2 

A= [2,4,7,5,6]
s= [2,4,7,5,6]
```
```
if(iL > m) A[k] <- S[iR++]
else if(sil < sir )A[k] <- S[iL++]
else A[k] <- s[iR++]
```


T(n) be the time 

take time Theta(N) step 1
tstep  2 takes time T(n/2 ) + T(n/2 )
step 3 takes Theta(n)

T(n) = {
    T(n/2 ) + t(n/2 )+ Theta(N) or Cn when n > 1 
    theta(1) when n = 1
}

replace Theta's, Theta(n) = Cn 

growth rate T(n) in Theta(n logn )

T(n) = 2 T(N/2) + cn n>1 

T(1) = C

n = 2^k?

T(2^k) = 2 T(2^k - 1 ) + c2^k
= 2 (2 T(2^k - 2) + c 2k^-1) + C2^k

= ...

= 2^k T(2^k-k ) + kc2^k // T(1) = C

T(2^k) = 2^kc + kc2^k
= c g^k(k+1)
since n = 2^k, k = log(N)
T(N) = c n(log(n) + 1 )
in Theta(nlogn)




sum i = 0, n - 1 2^i = ?

2 ^ (K + 1) - 1 

End of module 1 

### Module 2 

Priority Queue with Heap 

Queue ADT where you can 

enqueue inserting an item

dequeue removing the least recently inserted item

FIFO order 

enter at the rear, removed from the front 

Linked lists or circular arrays/ arrays

Priority Queue : 

Insert 

Deletemax 

maximum oriented or minimum oriented 

Priority Queue to Sort 

insert PQ to empty PQ 

PQ.insert everything 

PQ.deleteMax() everything, n-1 down to 0 

O(n + n * insert + n * deleteMax)

PQ with unsorted array 

```
A = [2,7,4 ...]

A = [2, 7, 4 , 1..]
```

insert O(1)

deleteMax(O(n))

## Lecture May 21

Insert K in array A

1. if Full, copy it into new array
2. insert 

cost of nth insert 
1. 1+n if n power of 2
2. 1 other wise 

Amortized over all insertions then take O(1) extra time 

Total # of copies for insert 1,2,3,4.. n a paower of 2 

= 1 + 1 + 1... n times + 1 + 2 + 4 + 8//s

= n + (2n - 1) 

so it is indeed Theta(n)

PQsort with unsorted array becomes selection sort 

```
using a sorted array 

A = [1,2,7]

insert 6 A = [1,2,6,7]

then need to move anything beyond 6

i.e insert would be linear time 

O(n) 

deleteMax: sorted so just O(1)
```

the height of a binary tree is the length of the longest path from root to a node 

binary heap 

example heap 

only show priorities show them directly in the node 

see lecture slide 

All the levels of a heap are completely filled Except the last one 

filled items in the last level are left justified 

heap order - for any node i, the key of the parent of i is larger than or equal to key of i 

2^h <=  n <= 2^h+1 - 1

heaps should not be stored as binary trees 
just go from left to right as A0, A1 ... 

for a node i 

left child 2i + 1 
right child 2i + 2 
parent of node = (i - 1) / 2 

last node is n - 1 

use helper functions to hid implementation 

fix up 

add the node in the last node and then move up if needed 

fixup(A,k)
k is an index corresponding to a node of the heap 
while parent(k) exists and A(parent(k)) < Ak do
    swap Ak and Aparent
    k <- parent
bubbles up until it reaches correct place in the heap 

O(logn) since O(height of heap);

expanding the array 

if n = 3 number of nodes >= 2^n = 8
<= 16 - 1 

any n, number of nodes >= 2^n <= 2^h+1 - 1 

Delete max 

replace root by last leaf and last leaf is taken out 

then perform a fix down 

while k is not a leaf 

find larger child j 

if Ak >= Aj break 

swap Aj and Ak

j becomes new k 

PQsortWithHeaps(A)

O(n + n* insert + n * delete Max)

runtime is O(n logn)

Heaps can be built faster if we know all input in advance 

same input array for input and heap O(1) auxilliary space 


## lecture May 23 

building heaps 

n items, build heap containing all of them 

Top down heapify 

for i = 0 to n - 1 do bubble up at position i 

lower bound on worst case 

each node bubble up, height of tree log n

bottom up heapify 

O(N)

heapSort: from i from n - 1 down to 1 do 

swap 

bubble-down 

so O(n log n)



Absent until Finding the largest items;

Kth largest item in an array A of n distinct numbers

Make K passes, deleting max num each time = Theta(kn)

sort A then return A[n - k] Theta(nlogn)

Scan array and maintain K lagest numbers seen so far in a min heap :

Theta(n log k)

create a max heap with heapify A 

call deleteMAx(A) k times 

Theta(n + k log n);

heap can be built in linear time 

x / logx is increasing 

k/log k = f(k) <= f(N) = n/log n 

k log(n) <= n log k 


### Module 3 Sorting and Randomized Algorithm 

QuickSelect 

Randomized Algorithms

QuickSort

Lower Bound for Comparison-Based sorting 

NOn-Comparison-Based Sorting 

Best heap based for selecting k th largest element = n + k log n

for median finding, Theta(n log n)

same cost as our best sorting algorithm 

Crucial Subroutines 

quick-select and quick-sort rely on two subroutines 

choose-pivot(A) choose P, use pivot value v <- A(p) to rearrange the array 

partition(A, p)

## Tutorial 

Delete Max -- bubble up 


Q2 Describe a data structure D that maintains a set of numbers under following operators 


Insert(D,a) add a into D O(logn )

Median(D) returns the element with sorted index D/2 O(1) time 


Idea


a0, a1 .... a n/2 - 1 and a n/2 - 1 ... a n - 1 

store ther first in a max heap, and sore the last few in a min heap 

calculate the max and min node's average 

when adding a into D 

either a, a n/2 - 1 a n/2 a n/2 + 1 becomes the new median 

if Hl != 0 then return h l root 

Insert(D, a)

if |hl| = 0 

then Hl.insert(A)

else if hL root <= a,

hl(insert a)   
    if Hl > hs + 1 
    Hs.insert(hl.deleteMax)
return D

if|hs| = 0 then Hs.insert(A)

if Hs.root >= a
    hs.insert(a)
    if |Hs| > hL + 1 
    Hl.insert(Hs.deleteMean()) -- so basically balancing the heap 

Hl.insert(a)

if |Hl| > |Hs| + 1 then 
    Hs.insert(Hl.deleteMin());


Q3 Give k sorted arrays  A1 .. A b

where the combination of these arrays has n elements 

give O(n log k ) time for a sorted arra containing all of those elements 

heap cannot contain n but k elements 


A1 x x1 x2

A2 y y1 y2... 


Smallest element is one of the 2. Supposed Y smallest, what is 2nd smallest?

One of x or y'

the possibility you need to maintain is just the smallest elements of each array currently

entries as a triple 

set {x, y, z} => {(x, 1 ,0), (y,2,0)}

```

H <- min heap S[0..n-1 ]

for i = 1 to k 
    H.insert((A[0], i, 0)) \\O(k log k)
for l = 0 to n = 1 
(a,b,c) <- H.deleteMin();
s[l] <- a 

if a < |Ab| - 1 
    H.insert(Ab[c + 1], b, c +1 )


```


## Lecture May 28

quick sort 

choose pivot and partition(A, p)

linear time impelmentation of partition 

make those into smaller, equal and larger 

compare those elements with Ap

then overwrite the original A by 

efficient in place partition Hoare

worst case for quick select -- if it's already sorted 

then you ned to proceed one by one 

T(N) = T(n-1 ) + cn or c 

So Tn in O(n^2)

best caase analysis: no chhosen pivot, total cost Theta(N)

average case is sum of costs for all permutations, divided by n!

Tavg(3,k) = 1/6 (T(0,1,2, k) + T(0,2,1,k) + ....);

= 1/n(cn) + 1/n sum i -> k - 1 (cn + T(n - i - 1, k - i - 1)) + 1/n sum k+1 -> n (cn + T(i, k))

proof for i = 0 ... n - 1 a fraction of 1/n of all permutations has pivot index i 

average runtime for these permutation is the 3 respectively 

prove T(N,k) <= 4cn for n >= 1 by induction 

by induction on n 

T(n,k) = cn + 1/n(sum i -> k - 1 (cn + T(n - i - 1, k - i - 1)) + sum k+1 -> n (cn + T(i, k)))

<= cn + 1/n (sum i=0 -> k-1 4c(n - i - 1) + sum k+1 -> n 4ci )


T(n, k) <= cn + 4c/n (sum 0 to k - 1 n - i - 1 + sum k + 1 to n - 1 i )

<= 3/4n^2 for the last part 

T(n, k) <= cn + 4c/n * 3/4 N^2 = cn + 3cn = 4cn 


## Lecture May 30

Average case analysis of quick sort 1 

T(N) be average in a size n array 

(n-1)! pivots have pivot index i 

so average running time = 

1/n! * sum i to n - 1 running time for instance i 

<= c * n + ...

See slide 

Theorem T(N) in nlogn 

prove TN <= 2cn log(n) by inductino onto n 


1/n! sum of permutations of T(I)

Lower bound for Comparison-Based Sorting 

N log n 

sorting is definitely more or equal to nlogn if you compare

redix is O(N)


each leaf in decision tree is labelled by a permutation 

depth of leaf is number of comparisons the algo does for this permutation 

n! leafs in the tree

log(n!) <= h + 1 

n! < 2^h+1 - 1 <= 2^h+1

log(n!) is Theta(n log n) => h is Omega(n log n)



## Tutorial 6/3 

Sorting lower bounds, expected case analysis 
```
Q1. Let 0 < epsilon < 1 

supposed that A is an array of n items such that the first a(n) = n - [n ^ epsilon] items are already sorted

Describe an O(n) time algorithm to sort A

A1. 

i.e from 0 to an sorted, 

then the later on unknown 

key idea is that the array from index A[a(n) to n - 1 ] can be sorted in O(n) time

To sort an array length k using merge sort, O(k log k) time

for us .. sorting A[an to n-1] uses O(n^epsilon log n^epsilon)

T(n) <= c n^e log n^e
<= 2n^e * c log(2 n^e) since [n^e] <= n^e + 1 <= 2 n^e 
= 2n^e * c * log(n) + 2n^e * c

Then use lhopital's rule 

lim T(n) / n 

take derivative 

2 e c logn + 2 c / n ^ 1-e 

.. = 0 in the ned 

```

Algorithm 

sort the two portions in O(n) as described 

then merge them together 

this takes O(a(n) + n - a(n)) = O(n)


```
suppose that we have n devies nad n chargers 

each charger fits with 1 device, and we would like to pair the chargers with their corresponding devices

only compare a device with charger and the out come is either too small or match or too large 

show that any algo is at least Omega(n log n )

can think of a solution to this problem as a permutation of the chargers ordering of the devices has been fixed 

number of solutions = number of permutations of n = n!

think of an algorithm as a decision tree 

internal nodes contains paris(i,j) signifying that the algo with chargers 

leaf nodes correspond to solutions the algo needs to return

A bound on the height of this decision tree corresponds to a bound on the worst case # of comparisons made 

height as being the longest possible flow of execution of the algorithm

bound # of leafs  n! <= # of leafs <= 3^h

solve for h  n! <= 3^h
log3 n! <= h

log n! is also in Omega(n log n)

thus h is Omega(n log n) as well


```

Q3

ANalyze worst case expected runtime of BogoSort(A) 

shuffle, if sorted return, or else bgo

supposed Shuffle(A) takes O(n)

Te(n) = cn * 1 + dn*1 + d1*(1/n!) + Te(n)(1 - 1/N!)

Te(n) / n! = c+d n + d' /n!

Te(n) = c + d n * n! + d' belongs to O(n n!)


## Lecture 

Claim let z be the first non balanced node we find after insert 

let T be the subtree rooted at Z 

let T' be T after restructure 

Ex. 7 - 6 - 5 

    6
5       7

Then 1. T' is balanced 

height of T' is height of T before insert 

## Lectuer 

Delete in skip lists

Skip list 

basically accessing sentinel first and then go through the linked lists 

Proof 

roughly 2n space

probability of (tower #j has height >= i) =Prob(HH....HT)

increase height of skip list to have h > i levels

use getPredecessors(k) to get stack P 

E(#  of elements in tower j at level i)

1 * 1/2^i + On(1-1/2^i)


E(# of elements in tower J) = 2 


Re-ordering items

Unordered list/array implementation 

make search more effective?

frequency of access


## Tutorial

Left rotation 

right rotation 

double left rotation 

Z
 \ 
  Y
/ 
x 

is when you do left rotation

thne it becoems Z X Y  with X in the middle 


Q1 perform insert 60 on T and afterwards delete 72 

         64
    18        80
  16  32   72    96 
14  22|  |48 
        |35 |50

left rotation on 32 

Q2 

We will show that deleting a node in an AVL tree of height H may require Theta(H) rotations to fix 

Define a family Th h>= -1 

rerursively T -1 is empty and T0 is a single node 

for h >= d Tn has the form 

    o

th-2 th-1 



Qa what is height of Tn 

pf by induction it has height i 

h(T) is height of T

base case T-1 has height -1 T0 has height 0

Assume Tj has height j for j < i 

assume Tj has height j for j < i 

Ti has height 1  + max T i - 1 T i - 2

= i 

b) argue that Tn is an avl tree proof by induction on Ti 

Base case 

Hypothesis Supposed Tj is an AVL tree for all j < i 

Then consider Ti 

by induction Ti - 1 and T i -2 are AVL trees 

balance factor = 1 

so Ti is an AVL tree

if delete a, then just 1 left rotation 

repeat with T3 

there is a leaf in Th that when deleted requires h/2 rotations to fix 

proof by induction 

T2 and T3 is proven

supposed claim is true fro all j < i 

consider Ti 

In total we require i - 2 /2 + 1 rotations 

= (i/2) 


## Lecture 

For an initial ordering I let C:I,k be the expected cost of the kth search 

starting from I 

then for K -> infinity C,I,k -> SumP i to n : ( 1 + sum j!= i pj/(pi+pj))

COPT <= C MTF <= 2 COPT

Transpose Heuristic 

lower bound for search 

comparison model: minimum log n

what about special keys?

decision tree for sorting, reuse here 

height of decision tree is h >= log(n+1)

height of decision tree determine worst case number of comparisons that need to be made

## Lecture 

If T is a tree where every internal node has at least 2 children

number of internal nodes(T) <= #leaves(T) - 1

proof by induction on the number of nodes in the tree

proof by induction on the number M of nodes in T

T = 0

in T = 0

lT = 1 

good 

Assume true for trees with 1.... M - 1 nodes

Prove it for M 

T = Root and Some children

assume m >= 2

T1... Tm satisfy the induction assumption

internal nodes in T

root internal nodes in T1.. internal nodes in Tm

in(T) = 1 + in(T1) +...+ in T(m)

leaves in T : leaves in T1 .... leaves in Tm

L(T) = L(T1) + ... + l(Tm)

in(T) <= 1 + (l(T1) - 1) + ... + l(Tm) - 1

<= l(T1) + l(tm)  - m + 1 

(-m + 1) <= 1 because 2 <= m

<= l(T) - 1

## lecture 

Uniform hashing assumption 

suppose we have keys k1 to kn in a table of length M 

then for any key k, prob(h(k) = j) any j in O...M = 1/M

UHA, anykey Ki is expected to collide with (n - 1)/M other keys, close to alpha

E(number of collisions) for ki = sum of Xij

independent 

Probability(i = a, j = b) = P(a)*P(b)

E(x,ij) = Prob(hki = h hkj = 0) * all the things together 

so 1 / M * 1 / M ... + 1/M * 1/M uniform

= 1/M

E(x ij) = 1/M

E total colision sum j!=i n-1/M

## Tutorial

draw compressed tries containing the keys 


Q2 have access to a boolean function Query(K) which returns true iff k < n  

obtain an upperbound on n 

k = 1 

while query is true 

k = 2k 

k/2 < n <= k 

use binary search 

low = k/2 high = k 

while low < high 

mid = 1/2 low + high 

if query mid is true 
low = mid + 1 
else high = mid 

analysis:

require O(log(2^k - 2^k-1 + 1))
= O(kig(2^k-1 + 1 ))
 
=O(k') = O(log n)

n <= 2^k' 

gallop search, each double the time 

## lecture 

Range searching in Dictionaries 

search(k) look for one specific 

all items fall within a given range 

RangeQuery(18,45) returns 19,33,45

S be the output size 

need Omega(S) to report the items 

sometimes s = 0 and s = n, so keep it as separate parameter 

for sorted array, can be done ine O(log n + s ) times 

binary search find i and i' 

binary search trees 

multi dimensional data 

d aspects 

aspect values are numbers 

query rectangle A 

store a 1 dimensional dictionary or quad tree 

width height of R should be a power of 2 

structure:

root r of quad tre is associated with region R 

if R contains 0 or 1 points root r is a leaf that stores point 

else split 

R is 0 16 * 0 16 

1 2 3 4 quadrant in order

then P4, 0,8 * 8 16, 0 8 * 0 8, p 5 

search insert delete

search for the point, split the leaf while there are two points in one region

delete leaf until parent node with one more children 

B(S) is sidelength of R / min distance btween points in S

h is in theta(logB(s))

complexity of building = theta(NH)

theta(Nh) worst case 

KD tree - split horizontally or vertically alternating 

each have roughly same number of nodes 


The partitions/splits are placed at the median of all the points inside the node. Calculating the median can be done in O(n)O(n) time, but is rather difficult to do in practice. Leaf nodes are those that have no split, and store points.

Check condition x < x(a).x? yes or no split 

and then y < x(b).y check 

smaller vs larger or equal 

always puts n/2 on each side 

h(n) belongs to O(logn)

so build in Theta(n logn)

## Lecture

 BST range search

 search is O(2 log n + s) = O (logn + s)

## Tutorial

CS240 Tutorial 9
```
Q1 build a quad tree over the plan [0,8), [0,8) using the points 

1,4 2,5 3,2 4,7 7,3 6,1 5,6 and 3,7 

points on split lines belong to the right/top section 

recursively 

sub divide plane into 4 //if null, still put in if any child is there

                    root
    SW(3,2) SE[4,8),[0,4)        NE [4,8), [4,8)          NW [0,4), [4,8)

        SE(6,1) NE(7,3)     NW [4,6), [6,8)         SW[1,4) SE (2,5), NE(3,7)

                        SE(5,6) NW(4,7)  
```


Instead of dividing into 4 equal pieces

Ensuring same number of points 

algorithm alternates 

x < 4? 
Y?              N?
Y < 5?         
Y       N 
X < 3? x <3? 
Y   N   Y   N 
1,4 3,2 2,5  3,7

2D range tree 

23,13 49,23 31,19 28,37 19,67 13,43 79,89 82,2 7,7 10,31 70,5 68,17 1,3 44,11 32,47 

Draw the X bst 

            31,19
13,43       68,17

7,7 23,13   49,11 79,89

etc... 

Draw Y bst 

each node in X bst contains a Y coord BST with all its children 

To perform a range rearch for 23,75 and 8,20 

Do a ID range search of X on 23,75 

search for 23, search for 75

## lecture 

DFA are preprocessing of patterns 

Karp-Rabin

Boyer-Moore

eliminate guesses based on completed matches and mismathces 

if hash matches then check explicitly 

update fingerprints in const time 

remove contribution of the leading digit, 
shift one and add a new digit 

big O(m + n)

