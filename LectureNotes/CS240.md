# CS240 Notes

## Lecture 1 

Assignment 0 due May 15th 

Eric Schost 

Admin stuff:

50% Final

25% Mid Term

5 assignment each 5% 

ADT, abstract data type

how to implement efficiently using DS

mathematical analysis 

psuedo-code, analyzed using order notation 

big Oh
 
priority QUeue, heaps 

sorting, selection 

AVL trees, B trees 

skip lists 

hashing 

quadtres, KD-trees

range search 

tried 

string matching 

data compression 

Problem, count positive integers in an array

An instance : [1,2,3,4,5]

solution: 15

size of an instance(length of the array)

Initialize a counter to zero, one pass over array, for any entry A[i] if A[i] > 0, increment counter 


or 
```

count(a)

res = 0 
for i = 0... n-1,
if A[i] > 0
    res ++
return res

```

running time and auxiliary space 

amount of time/memory are dependent on Size(I)

Running Time Analysis:

No implementing

Independent of Environment

All input instances 

Random Access Machine MOdel

idealied model 

order notation 

ignore constants and lower order terms 

Order Notation 

F(n) = 75n + 500 
g(n) = 5n2

1. for n >= 20 25*20 <= 25n
2. for n >= 20 20n < n2 -> 100n <= 5n2

first principle proof

2n2 + 3n + 11 <= cn2 for all n >= n0

for n>=1 2n^2 < 2n^2 

3n <= 3n^2

11 <= 11n^2

f(n) <= 16n^2 = 16g(n)

so, taking n0 = 1 c = 16, this proves f(n) belongs O(g(n))

## Lecture 2 5/9

find C and n0 such that it is satisfied 

omega notation exists c > 0 n0 > 0 such that c|g(n)| <= \f(n)\

theta notation 

theta means f(n) is both O(gn) and Omega(gn)

tight bound 

omega is lower bound 

f(n) belongs to Omega(g(n))

there exists C and n0 such that both > 0 

for n >= n0 f(n) >= c g(n) lower bound on run time 

theta means both omega and Big O 

prove f(n) 2n2 + 3n + 11 belongs to Omega(n2)

for all n >= 1 2n^2 >= 2n^2
3n >= 0
11 >= 0

f(n) >= 2n2 = 2g(n)
taking c = 2 and n0 = 1,that proves f(n) belongs Omega(n)

1/2 n^2 - 5n belongs omega(n^2)

f(n) = 1/2 n^2 -5n
g(n) = n^2 

let n >= 20 n^2 >= 20n 
1/4 n^2 >= 5n 
```
-5n >=  -1/4 n^2
1/2n^2 - 5n >= -1/4 n^2 + 1/2n^2 i.e 1/4 n^2 

```
f(n) >= 1/4 g(n) absolute values 

therefore by FP, Omega true 

Taking n0 = 20 and c = 1/4 thus proves Omega 

prove logb(n) belongs to theta(log n) for all b > 1 from first principles 

f(n) = logb(n) log(n)/log(b)
g(n) = log(n)

1/logb g(n) <= f(n) <= 1/logb g(n) taking n0 = 1, c1 = c2 = 1/logb 

strictly smaller larger asymptotic bounds 

small o notation 

f(n) belongs o(g(n)) if for all constant c > 0, ther eexists a constant n0 > 0 such that ... 

small omega notation 

f(n) belongs to small(g(N)) if g(n) belongs to o(f(n))

reverse of small o 

little o example 

f(n) 2000n^2 

g(n) n^n 

f(n) is little o of g(n)

given c > 0 find n0 such that n > n0 2000n^2 < cn^n

2000 < c n^n-2 

for n >= 3 n-2 >-1 so n <= n^n-2

for n>= 3 and n >= 2000/c + 1 

2000/c < 2000/c + 1 <= n <= n^n-2 

so 2000 < cn^n-2 

takig n0 = max(3, 2000/c + 1 )
thus proves ....

Algebra of Order notations 

f(n) belongsto Theta(f(n))

Maximum rules 

O(fn + gn) = O(max(fn, gn))
Omega(fn + gn) = Omega(max(fn, gn))

Transitivity 

fn = O(gn), gn = O(hn) => fn = O(hn)

similar with Omega
 
Techniques for Order notation 

L = lim n->infinity fn/gn 

f(n) > 0 and g(n) > 0 for all n >= n0

then f(n) is 

o(gn) if L = 0
Theta(gn) if 0 < L < inifinity 
small omega(gn) if L = infinity 

Lhopital's rule 

sufficient conditions 

let fn be a polynomial of degree d >= 0

cd n^d + ..  + c0

for some cd > 0

f(n) belongs to Theta(n^d)

fn/gn = cd + cd1 * 1/n ... 

all goes to 0 except cd, a constant

so theta(gn) is true

prove n(2 + sin n pie /2 ) is theta(n)

limit does not exist

for n >= 1 

-1 <= sin(n pie / 2) <= 1 

n <= n(2+sin(...)) <= 3n 

//first principle proof

compare the growth rate of log n and n 

logN ^ c and n ^ d 

f(n) = log(n)
g(n) = n

want f(n) belongs to o(g(n))

limit test 

L'hopital 

f(n) = ln(n) / ln(2) so f'n = 1/nln(2)

g'(n) = 1 

## Tutorial 1 

Latex and Asymptotic Analysis 

Zach TA 

zfrenett@uwaterloo.ca

pdflatex

TexStudio or Miktex

```
\documentclass[12pt][article]

\usepackage[]

amsmath, amsfonts
graphicx 
tikz for trees 


\begin{document}

\\ newline 

\large
\bf bold font

\vspace vertical space
\hspace horizontal space 

\section*{abc} section not numbered
\setion{abc} numbered 


$ \log(n!) \in \Theta(n \log n) $ \\ inline math 
\Theta vs \theta or \omega vs \Omega
\exists 
\forall


\begin{equation*} without number with *
\end{equation} 

$ or \begin{equation}

\lvert \rvert vertical line right and left 

\leq left equal 
\geq 

c_1 subscript for c with 1
c^1 means super script
\sum_{i = 1}^{n} for the summation 


\begin{align*} for aligning multiple equations

& for aligning equations
&= 

\prod product 


```

prove from first principels that

$$ 2^{\sqrt{logn}} \in o(n) $$

show that forall c > 0, exists n0 > 0, such that 
$$ |2^{\sqrt{logn}}| < c|n| \hspace{1cm}  \forall n >= n0 $$

prove 
$$2^{\sqrt{logn}}| < \sqrt{n} < c|n| $$

for n >= 17

16 < n 

4 leq logn
2 leq sqrt(logn)

2 sqrt(logn) < log(n)



 2^\sqrt{logn} < 2^log_4{n} = sqrt(n) 

 

## Lecture 4 


$$ log(n) \in o(n) $$ 

similarly, 

$$ log(n) \in o(n^a) $$
for a > 0

f(n) = log(n) => f'(n) = 1/ln2 * 1/n

g(n) = n ^ a  => g'(n) = ...

so f'(n) / g'(n) = 1/ ln2 * 1/a * 1/n^a

lim n -> infinity f'n/g'n = 0 ...


finally, we prove 

$$ log(n) ^ c \in o(n^d) \hspace{1cm} for c,d > 0 $$

$$ log(n)^c/n^d = log(n)^c / n^{(d/c)^c} $$

$$ = (log(n) / n^{d/c})^c $$

in the previous case with a = d/c 

proved 

$$ lim \infin log(n)/n^{d/c} = 0 $$

so lim.. = 0 

by limit test, 

log(n) ^ c in o(n^d)


little o for growth rate smaller 

theta the same 

small omega growth rate of f(n) is greater than growth rate of g(n)

common growth rate -- in lecture notes

### Analysis of Algorithm 2

nested loop

def T1(n) be the runtime of test1(n)

Then 

$$ T1(n) \in \theta(s1(n)) $$ 

where s1(n) is the number of times we go through step 4 

$$ s1(n) = \sum_{1 - n}^n \sum_{j = i}^n 1 $$

term 1 = n - i + 1 
term 2 outer loop sub it as n - i + 1 

= ... 

= n^2 - n(n+1)/2 + n
= 1/2 n^2 + 1/2n in O(n^2)

so T1(n) in theta(n^2)

s1(n) \leq sum 1 - n sum 1 - n (1) = n ^2 

so O(n^2) is confirmed 

s1(n) is number of integer points in ..

get omega from 1/4 N^2 

```
s1(n) >= sum 1 - n/2 sum i - n 1 

>= sum 1 - n/2 sum n/2 - n 1 >= n^2/4

thus omega(n^2)
```

O() and omega() proves s1(n) belongs to theta(n^2)

T2(n) belongs to theta(S2(N))

S2(n) is the number of times we go to step 6



then...


S2(n) = 

$$ \sum_{i = 1}^n \sum_{j = i}^n \sum_{k=i}^j 1 $$
can prove s2(n) = ...... 

can be proven easily that s2(n) <= n^3 

and simplify 

s2(n) >= sum 1 - n/3 sum i - n sum i - j (i)

s2(n) >= sum 1 - n/3 sum 2n/3 - n sum i - j (i)

sum i - j >= n/3 if i <= n/3  and j >= 2n/3 

s2(n) >= (n/3)^3 

while loop determines Ta(I) 

Test2(A,n) sorts A in decreasing order theta(n^2)

worst case A is sorted in increasing order theta(n)


average case complexity of an algo


## Lecture 5 

Merge Sort 

A of n integers : 

split A into 2 sub arrays 

recursively run merge sort on A l and A r 

after Al and Ar sorted, use a function merge to merge them into a single sorted array 

see slide for Merge sort implementation 

Mergesort ... 

Merge  ..

both sub arrays sorted 

Merge taeks time Theta(r - l + 1) i.e Theta(n) time for merging n elements 

bounded 

```
n = 5 
l = 0 
r = n - 1 = 4 
m = l + r / 2  = 2 

A= [2,4,7,5,6]
s= [2,4,7,5,6]
```
```
if(iL > m) A[k] <- S[iR++]
else if(sil < sir )A[k] <- S[iL++]
else A[k] <- s[iR++]
```


T(n) be the time 

take time Theta(N) step 1
tstep  2 takes time T(n/2 ) + T(n/2 )
step 3 takes Theta(n)

T(n) = {
    T(n/2 ) + t(n/2 )+ Theta(N) or Cn when n > 1 
    theta(1) when n = 1
}

replace Theta's, Theta(n) = Cn 

growth rate T(n) in Theta(n logn )

T(n) = 2 T(N/2) + cn n>1 

T(1) = C

n = 2^k?

T(2^k) = 2 T(2^k - 1 ) + c2^k
= 2 (2 T(2^k - 2) + c 2k^-1) + C2^k

= ...

= 2^k T(2^k-k ) + kc2^k // T(1) = C

T(2^k) = 2^kc + kc2^k
= c g^k(k+1)
since n = 2^k, k = log(N)
T(N) = c n(log(n) + 1 )
in Theta(nlogn)




sum i = 0, n - 1 2^i = ?

2 ^ (K + 1) - 1 

End of module 1 

### Module 2 

Priority Queue with Heap 

Queue ADT where you can 

enqueue inserting an item

dequeue removing the least recently inserted item

FIFO order 

enter at the rear, removed from the front 

Linked lists or circular arrays/ arrays

Priority Queue : 

Insert 

Deletemax 

maximum oriented or minimum oriented 

Priority Queue to Sort 

insert PQ to empty PQ 

PQ.insert everything 

PQ.deleteMax() everything, n-1 down to 0 

O(n + n * insert + n * deleteMax)

PQ with unsorted array 

```
A = [2,7,4 ...]

A = [2, 7, 4 , 1..]
```

insert O(1)

deleteMax(O(n))

## Lecture May 21

Insert K in array A

1. if Full, copy it into new array
2. insert 

cost of nth insert 
1. 1+n if n power of 2
2. 1 other wise 

Amortized over all insertions then take O(1) extra time 

Total # of copies for insert 1,2,3,4.. n a paower of 2 

= 1 + 1 + 1... n times + 1 + 2 + 4 + 8//s

= n + (2n - 1) 

so it is indeed Theta(n)

PQsort with unsorted array becomes selection sort 

```
using a sorted array 

A = [1,2,7]

insert 6 A = [1,2,6,7]

then need to move anything beyond 6

i.e insert would be linear time 

O(n) 

deleteMax: sorted so just O(1)
```

the height of a binary tree is the length of the longest path from root to a node 

binary heap 

example heap 

only show priorities show them directly in the node 

see lecture slide 

All the levels of a heap are completely filled Except the last one 

filled items in the last level are left justified 

heap order - for any node i, the key of the parent of i is larger than or equal to key of i 

2^h <=  n <= 2^h+1 - 1

heaps should not be stored as binary trees 
just go from left to right as A0, A1 ... 

for a node i 

left child 2i + 1 
right child 2i + 2 
parent of node = (i - 1) / 2 

last node is n - 1 

use helper functions to hid implementation 

fix up 

add the node in the last node and then move up if needed 

fixup(A,k)
k is an index corresponding to a node of the heap 
while parent(k) exists and A(parent(k)) < Ak do
    swap Ak and Aparent
    k <- parent
bubbles up until it reaches correct place in the heap 

O(logn) since O(height of heap);

expanding the array 

if n = 3 number of nodes >= 2^n = 8
<= 16 - 1 

any n, number of nodes >= 2^n <= 2^h+1 - 1 

Delete max 

replace root by last leaf and last leaf is taken out 

then perform a fix down 

while k is not a leaf 

find larger child j 

if Ak >= Aj break 

swap Aj and Ak

j becomes new k 

PQsortWithHeaps(A)

O(n + n* insert + n * delete Max)

runtime is O(n logn)

Heaps can be built faster if we know all input in advance 

same input array for input and heap O(1) auxilliary space 

